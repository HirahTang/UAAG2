# Adam optimizer configuration
name: adam
lr: 5.0e-4
lr_min: 5.0e-5
weight_decay: 0.0

# Learning rate scheduler
lr_scheduler: reduce_on_plateau  # reduce_on_plateau, cosine_annealing, cyclic
lr_step_size: 10000
lr_frequency: 5
lr_patience: 20
lr_cooldown: 5
lr_factor: 0.75

# Gradient clipping
grad_clip_val: 10.0

# Decay settings
gamma: 0.975
